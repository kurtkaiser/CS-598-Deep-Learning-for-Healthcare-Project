2022-05-08 18:21:06,707 - INFO - Config:
2022-05-08 18:21:06,707 - INFO - {
    "L2_regularisation": 0,
    "alpha": 100,
    "base_dir": "models/experiments/eICU/LoS/Transformer",
    "batch_size": 32,
    "batch_size_test": 32,
    "batchnorm": "mybatchnorm",
    "d_model": 16,
    "dataset": "eICU",
    "diagnosis_size": 64,
    "disable_cuda": false,
    "exp_name": "Transformer",
    "feedforward_size": 256,
    "intermediate_reporting": false,
    "labs_only": false,
    "last_linear_size": 17,
    "learning_rate": 0.00017,
    "loss": "msle",
    "main_dropout_rate": 0.45,
    "mode": "train",
    "model_type": "tpc",
    "n_epochs": 10,
    "n_heads": 2,
    "n_layers": 6,
    "name": "Transformer",
    "no_diag": false,
    "no_exp": false,
    "no_labs": false,
    "no_mask": false,
    "percentage_data": 100.0,
    "positional_encoding": false,
    "save_results_csv": false,
    "seed": 674403789,
    "shuffle_train": false,
    "sum_losses": true,
    "task": "LoS",
    "trans_dropout_rate": 0
}
2022-05-08 18:21:09,577 - INFO - Experiment set up.
2022-05-08 18:21:09,603 - INFO - Transformer(
  (relu): ReLU()
  (sigmoid): Sigmoid()
  (hardtanh): Hardtanh(min_val=0.020833333333333332, max_val=100)
  (trans_dropout): Dropout(p=0, inplace=False)
  (main_dropout): Dropout(p=0.45, inplace=False)
  (msle_loss): MSLELoss(
    (squared_error): MSELoss()
  )
  (mse_loss): MSELoss(
    (squared_error): MSELoss()
  )
  (bce_loss): BCELoss()
  (empty_module): EmptyModule()
  (transformer): TransformerEncoder(
    (input_embedding): Conv1d(176, 16, kernel_size=(1,), stride=(1,))
    (pos_encoder): PositionalEncoding()
    (trans_encoder_layer): TransformerEncoderLayer(
      (self_attn): MultiheadAttention(
        (out_proj): NonDynamicallyQuantizableLinear(in_features=16, out_features=16, bias=True)
      )
      (linear1): Linear(in_features=16, out_features=256, bias=True)
      (dropout): Dropout(p=0, inplace=False)
      (linear2): Linear(in_features=256, out_features=16, bias=True)
      (norm1): LayerNorm((16,), eps=1e-05, elementwise_affine=True)
      (norm2): LayerNorm((16,), eps=1e-05, elementwise_affine=True)
      (dropout1): Dropout(p=0, inplace=False)
      (dropout2): Dropout(p=0, inplace=False)
    )
    (transformer_encoder): TransformerEncoder(
      (layers): ModuleList(
        (0): TransformerEncoderLayer(
          (self_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=16, out_features=16, bias=True)
          )
          (linear1): Linear(in_features=16, out_features=256, bias=True)
          (dropout): Dropout(p=0, inplace=False)
          (linear2): Linear(in_features=256, out_features=16, bias=True)
          (norm1): LayerNorm((16,), eps=1e-05, elementwise_affine=True)
          (norm2): LayerNorm((16,), eps=1e-05, elementwise_affine=True)
          (dropout1): Dropout(p=0, inplace=False)
          (dropout2): Dropout(p=0, inplace=False)
        )
        (1): TransformerEncoderLayer(
          (self_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=16, out_features=16, bias=True)
          )
          (linear1): Linear(in_features=16, out_features=256, bias=True)
          (dropout): Dropout(p=0, inplace=False)
          (linear2): Linear(in_features=256, out_features=16, bias=True)
          (norm1): LayerNorm((16,), eps=1e-05, elementwise_affine=True)
          (norm2): LayerNorm((16,), eps=1e-05, elementwise_affine=True)
          (dropout1): Dropout(p=0, inplace=False)
          (dropout2): Dropout(p=0, inplace=False)
        )
        (2): TransformerEncoderLayer(
          (self_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=16, out_features=16, bias=True)
          )
          (linear1): Linear(in_features=16, out_features=256, bias=True)
          (dropout): Dropout(p=0, inplace=False)
          (linear2): Linear(in_features=256, out_features=16, bias=True)
          (norm1): LayerNorm((16,), eps=1e-05, elementwise_affine=True)
          (norm2): LayerNorm((16,), eps=1e-05, elementwise_affine=True)
          (dropout1): Dropout(p=0, inplace=False)
          (dropout2): Dropout(p=0, inplace=False)
        )
        (3): TransformerEncoderLayer(
          (self_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=16, out_features=16, bias=True)
          )
          (linear1): Linear(in_features=16, out_features=256, bias=True)
          (dropout): Dropout(p=0, inplace=False)
          (linear2): Linear(in_features=256, out_features=16, bias=True)
          (norm1): LayerNorm((16,), eps=1e-05, elementwise_affine=True)
          (norm2): LayerNorm((16,), eps=1e-05, elementwise_affine=True)
          (dropout1): Dropout(p=0, inplace=False)
          (dropout2): Dropout(p=0, inplace=False)
        )
        (4): TransformerEncoderLayer(
          (self_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=16, out_features=16, bias=True)
          )
          (linear1): Linear(in_features=16, out_features=256, bias=True)
          (dropout): Dropout(p=0, inplace=False)
          (linear2): Linear(in_features=256, out_features=16, bias=True)
          (norm1): LayerNorm((16,), eps=1e-05, elementwise_affine=True)
          (norm2): LayerNorm((16,), eps=1e-05, elementwise_affine=True)
          (dropout1): Dropout(p=0, inplace=False)
          (dropout2): Dropout(p=0, inplace=False)
        )
        (5): TransformerEncoderLayer(
          (self_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=16, out_features=16, bias=True)
          )
          (linear1): Linear(in_features=16, out_features=256, bias=True)
          (dropout): Dropout(p=0, inplace=False)
          (linear2): Linear(in_features=256, out_features=16, bias=True)
          (norm1): LayerNorm((16,), eps=1e-05, elementwise_affine=True)
          (norm2): LayerNorm((16,), eps=1e-05, elementwise_affine=True)
          (dropout1): Dropout(p=0, inplace=False)
          (dropout2): Dropout(p=0, inplace=False)
        )
      )
    )
  )
  (diagnosis_encoder): Linear(in_features=293, out_features=64, bias=True)
  (bn_diagnosis_encoder): MyBatchNorm1d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  (point_los): Linear(in_features=145, out_features=17, bias=True)
  (point_mort): Linear(in_features=145, out_features=17, bias=True)
  (bn_point_last_los): MyBatchNorm1d(17, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  (bn_point_last_mort): MyBatchNorm1d(17, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  (point_final_los): Linear(in_features=17, out_features=1, bias=True)
  (point_final_mort): Linear(in_features=17, out_features=1, bias=True)
)
2022-05-08 18:35:00,110 - INFO - Custom bins confusion matrix:
2022-05-08 18:35:00,110 - INFO - [[ 767919 1043424  173014   60110   27426   13516    7314    4072    5670
      538]
 [ 325953  621482  154395   65186   32591   16823    9103    5124    7132
      662]
 [ 149384  364559  119929   57784   30148   16220    8872    5044    7409
      644]
 [  77186  226689   92139   48524   26435   14442    8084    4706    7162
      663]
 [  43690  151187   70867   40047   22430   12459    7352    4400    6208
      636]
 [  27847  104738   55672   32848   19107   11102    6297    3802    6021
      621]
 [  17782   76733   45016   27119   16282    9501    5703    3381    5351
      572]
 [  11769   57359   36121   22975   13929    8253    4975    3060    4787
      513]
 [  29950  163792  115978   79007   50160   30554   18843   11575   19523
     2444]
 [  13899   88543   74406   55308   37187   24069   14978    9526   16645
     2267]]
2022-05-08 18:35:05,798 - INFO - Epoch: 0 | Train Loss: 113.3534
2022-05-08 18:37:32,980 - INFO - Custom bins confusion matrix:
2022-05-08 18:37:32,980 - INFO - [[152694 225967  38801  16472   9273   4603   1510    503    119      0]
 [ 54306 129697  35761  20331  13327   6684   2341    623    172      0]
 [ 22407  70965  25811  17881  13808   6932   2568    727    293      0]
 [ 10953  40281  18539  14214  12344   6636   2702    752    366      0]
 [  5946  24250  13125  11670  10356   6229   2618    952    396      0]
 [  3648  16202   9776   9223   8720   5704   2380   1091    404      0]
 [  2221  10719   7034   7351   7462   5101   2186   1018    368      0]
 [  1457   7745   5534   5655   6449   4305   2076   1022    382      0]
 [  2449  19522  16200  18993  23132  16364   8356   4242   1938      0]
 [   956   9227   8843  10789  15827  13148   7595   3665   1638      0]]
2022-05-08 18:37:34,494 - INFO - Epoch: 0 | Validation Loss: 98.9253
2022-05-08 18:51:20,524 - INFO - Custom bins confusion matrix:
2022-05-08 18:51:20,524 - INFO - [[912251 938705 139151  54568  26253  13586   7342   4205   6258    684]
 [350656 589539 145188  69701  36447  19760  10964   6146   9081    969]
 [151093 342109 116645  64533  35769  20220  11520   6641  10337   1126]
 [ 73784 206582  90914  54785  32084  18638  11060   6655  10352   1176]
 [ 40284 132854  69329  45126  28022  16698  10120   6033   9549   1261]
 [ 24400  89134  53905  37166  23885  14783   8975   5498   9121   1188]
 [ 15183  62727  42501  30987  20595  12822   7972   4996   8479   1178]
 [  9633  44773  33978  25962  17550  11204   7187   4481   7853   1120]
 [ 21698 118944 105956  88072  63126  41870  26894  17624  32570   5072]
 [  8969  54027  62352  59666  46428  32895  22279  15224  29777   5211]]
2022-05-08 18:51:26,193 - INFO - Epoch: 1 | Train Loss: 101.2033
2022-05-08 18:53:53,321 - INFO - Custom bins confusion matrix:
2022-05-08 18:53:53,322 - INFO - [[173803 208591  37538  14985   8183   4298   1604    628    312      0]
 [ 60257 123910  35931  20619  12315   6445   2514    863    388      0]
 [ 24394  68510  25928  18470  12722   6961   2683   1127    597      0]
 [ 11908  38770  18373  14779  11310   6841   2995   1104    707      0]
 [  6178  23345  13288  11592   9540   6416   3182   1167    834      0]
 [  3634  15637   9744   9240   8033   5637   2955   1301    967      0]
 [  2349  10235   6945   7126   6887   5142   2695   1290    791      0]
 [  1556   7269   5470   5620   5675   4458   2572   1194    811      0]
 [  2805  17995  15715  18552  20858  15981   9755   5812   3723      0]
 [  1029   8489   8045  10744  13362  12499   8840   4681   3999      0]]
2022-05-08 18:53:54,869 - INFO - Epoch: 1 | Validation Loss: 95.0477
2022-05-08 19:07:48,139 - INFO - Custom bins confusion matrix:
2022-05-08 19:07:48,140 - INFO - [[956009 903215 135732  52336  24820  12692   7062   4049   6289    799]
 [357694 578133 149459  71076  36158  19375  10462   6005   9032   1057]
 [151875 333576 121259  66515  36194  20363  11399   6723  10780   1309]
 [ 73135 200481  93585  56609  32966  18916  11154   6823  10957   1404]
 [ 39572 127686  70881  46489  28688  17252  10202   6317  10699   1490]
 [ 23683  85043  54323  38167  24613  15232   9345   5928  10239   1482]
 [ 14900  58947  42590  31638  21192  13237   8343   5348   9736   1509]
 [  9253  41348  34066  26511  18237  11617   7441   4789   9033   1446]
 [ 21400 107689 103199  88116  64701  43726  28990  19457  37590   6958]
 [  8382  48012  57647  57509  46434  34085  24198  16886  35792   7883]]
2022-05-08 19:07:53,991 - INFO - Epoch: 2 | Train Loss: 97.9898
2022-05-08 19:10:21,125 - INFO - Custom bins confusion matrix:
2022-05-08 19:10:21,126 - INFO - [[196032 194202  34022  13348   6946   3399   1260    495    238      0]
 [ 67487 122725  35662  19081  10575   4892   1927    585    308      0]
 [ 27169  69460  26718  17923  11149   5357   2224    928    464      0]
 [ 13422  39284  19515  14895  10067   5703   2325    999    577      0]
 [  6859  23855  14183  12145   8854   5519   2445   1024    658      0]
 [  3994  16035  10496   9649   7706   4918   2489   1111    750      0]
 [  2479  10563   7422   7666   6651   4669   2209   1128    673      0]
 [  1634   7466   5897   6083   5581   4015   2205   1033    711      0]
 [  3176  18707  16831  20158  19783  15019   9600   4814   3108      0]
 [  1262   8562   8801  11452  13197  11307   8287   4676   4144      0]]
2022-05-08 19:10:22,707 - INFO - Epoch: 2 | Validation Loss: 92.6888
2022-05-08 19:24:11,115 - INFO - Custom bins confusion matrix:
2022-05-08 19:24:11,115 - INFO - [[990534 867333 138721  52060  24138  12353   6904   3987   6073    900]
 [363172 568503 154944  71360  35647  18714  10111   5935   8927   1138]
 [151424 328901 126481  67435  36001  19834  11151   6696  10737   1333]
 [ 71752 196776  97783  56784  33067  19112  11237   6743  11268   1508]
 [ 38105 124725  73378  47026  28783  17599  10409   6551  11041   1659]
 [ 22857  82011  55470  39042  25065  15634   9571   5979  10731   1695]
 [ 14161  56337  43386  32267  21316  14016   8602   5461  10164   1730]
 [  8926  38968  34277  26568  18588  12119   7784   5051   9745   1715]
 [ 20636  98613 102926  89037  65691  45169  30121  20228  40861   8544]
 [  8302  42820  55196  57046  46839  35102  24911  17895  39141   9576]]
2022-05-08 19:24:16,871 - INFO - Epoch: 3 | Train Loss: 95.8244
2022-05-08 19:26:43,647 - INFO - Custom bins confusion matrix:
2022-05-08 19:26:43,648 - INFO - [[212410 178326  33732  13410   6928   3259   1186    477    214      0]
 [ 74249 116262  35508  19524  10482   4589   1818    538    272      0]
 [ 29700  66728  26965  18112  11219   5208   2107    928    425      0]
 [ 14367  38017  19751  15044  10338   5375   2269   1097    529      0]
 [  7317  22981  14291  12346   9063   5385   2543   1001    615      0]
 [  4205  15539  10423   9893   7868   4856   2468   1230    666      0]
 [  2581  10142   7472   7785   6854   4538   2252   1134    702      0]
 [  1709   7147   5878   6188   5727   4030   2133   1120    693      0]
 [  3468  17603  16760  19755  20207  15533   9739   4809   3322      0]
 [  1495   7858   8466  11168  13375  11753   8053   4977   4543      0]]
2022-05-08 19:26:45,192 - INFO - Epoch: 3 | Validation Loss: 91.0551
2022-05-08 19:40:36,799 - INFO - Custom bins confusion matrix:
2022-05-08 19:40:36,799 - INFO - [[1005907  850967  140779   51797   23887   12244    6636    3853    6021
      912]
 [ 365038  563280  158006   72134   35595   18551   10169    5832    8751
     1095]
 [ 150482  325507  129315   68447   36567   19810   11264    6749   10499
     1353]
 [  70008  195186   98726   58662   33439   19119   11150    6924   11190
     1626]
 [  36937  122642   74705   47972   29308   17477   10641    6571   11317
     1706]
 [  21693   80475   56292   39794   25444   15847    9857    5992   11000
     1661]
 [  13463   54855   43441   33040   22182   13748    8784    5639   10523
     1765]
 [   8361   37392   34573   27527   18659   12301    8110    5181    9857
     1780]
 [  19663   93599  101602   89802   66904   45897   30980   21589   42922
     8868]
 [   7720   39902   53497   56050   46905   35936   25713   18448   41756
    10901]]
2022-05-08 19:40:42,526 - INFO - Epoch: 4 | Train Loss: 94.3994
2022-05-08 19:43:09,936 - INFO - Custom bins confusion matrix:
2022-05-08 19:43:09,936 - INFO - [[226024 165386  33703  13458   6751   2936   1108    407    169      0]
 [ 80423 110587  35860  19674  10164   4230   1578    516    210      0]
 [ 32522  63880  27671  18290  10891   4977   1978    876    307      0]
 [ 15529  36949  20012  15221  10374   5213   2008   1040    441      0]
 [  7936  22196  14724  12580   9185   5138   2366    903    514      0]
 [  4614  15154  10570  10156   7880   4707   2421   1082    564      0]
 [  2850   9774   7734   7973   6877   4444   2151   1053    604      0]
 [  1888   6827   6196   6186   5864   3969   2052   1066    577      0]
 [  3848  16827  17356  20131  20741  15155   9789   4440   2909      0]
 [  1661   7426   8645  11450  13514  11826   8087   4760   4319      0]]
2022-05-08 19:43:11,510 - INFO - Epoch: 4 | Validation Loss: 90.1324
2022-05-08 19:56:58,698 - INFO - Custom bins confusion matrix:
2022-05-08 19:56:58,699 - INFO - [[1012088  846029  140286   51759   23538   12216    6508    3806    5923
      850]
 [ 365080  563660  158479   71575   35565   18533   10017    5758    8692
     1092]
 [ 148891  326692  129988   68304   36461   19882   11130    6781   10496
     1368]
 [  68868  194819  100349   57901   33445   19456   11376    6722   11522
     1572]
 [  35797  122461   74848   48247   29284   17696   10884    6664   11581
     1814]
 [  21138   79765   56456   39766   25513   15975   10112    6248   11337
     1745]
 [  12933   54029   43281   33475   22312   14220    8942    5684   10670
     1894]
 [   8101   36625   34259   27445   19103   12602    8169    5315   10282
     1840]
 [  19182   89532  100163   90537   67750   47565   31794   21717   44275
     9311]
 [   7308   37224   51658   55788   47172   36293   26497   19149   43912
    11827]]
2022-05-08 19:57:04,531 - INFO - Epoch: 5 | Train Loss: 93.4233
2022-05-08 19:59:31,680 - INFO - Custom bins confusion matrix:
2022-05-08 19:59:31,681 - INFO - [[225753 166686  32974  13472   6464   2864   1114    417    198      0]
 [ 79616 112449  35356  19510   9880   4036   1618    547    230      0]
 [ 31914  64932  27542  18264  10555   4950   2039    887    309      0]
 [ 15098  37763  19871  15073  10315   5079   2093    988    507      0]
 [  7736  22579  14642  12629   8981   5163   2355    923    534      0]
 [  4491  15398  10536  10126   7800   4642   2406   1172    577      0]
 [  2800   9915   7748   7962   6730   4371   2233   1020    681      0]
 [  1845   6840   6229   6218   5769   4019   1981   1074    650      0]
 [  3660  16865  17472  20100  20681  15052   9628   4723   3014      1]
 [  1639   7391   8657  11506  13264  11625   7803   4929   4874      0]]
2022-05-08 19:59:33,221 - INFO - Epoch: 5 | Validation Loss: 89.6324
2022-05-08 20:13:18,427 - INFO - Custom bins confusion matrix:
2022-05-08 20:13:18,427 - INFO - [[1013781  845766  139170   51831   23710   11862    6458    3712    5859
      854]
 [ 363890  565119  158466   72167   35516   18119    9748    5773    8563
     1090]
 [ 147480  327174  130322   68934   36629   19651   11333    6662   10483
     1325]
 [  67396  195285  100050   59120   33454   19477   11468    6904   11330
     1546]
 [  34950  122347   74616   48602   29949   17881   10918    6669   11597
     1747]
 [  20709   79548   55898   40216   25821   16192   10016    6421   11384
     1850]
 [  12720   53784   43036   33417   22437   14266    9048    5872   10909
     1951]
 [   7935   36231   33595   27704   19496   12628    8415    5371   10419
     1947]
 [  18719   87028   98691   90644   67725   47691   32707   22490   46232
     9899]
 [   6913   35043   50295   54852   47551   36673   26859   19403   46031
    13208]]
2022-05-08 20:13:24,099 - INFO - Epoch: 6 | Train Loss: 92.6005
2022-05-08 20:15:51,732 - INFO - Custom bins confusion matrix:
2022-05-08 20:15:51,733 - INFO - [[231934 163070  31219  13116   6101   2742   1091    448    221      0]
 [ 82297 112273  34155  18786   9400   3852   1674    527    278      0]
 [ 32904  65523  27090  17689   9982   4912   2136    762    394      0]
 [ 15701  38237  19645  14710   9797   5095   2064    968    568      2]
 [  8074  22971  14613  12391   8604   4933   2432    917    603      4]
 [  4671  15835  10405   9914   7563   4543   2423   1108    679      7]
 [  2929  10113   7791   7839   6530   4201   2219   1024    813      1]
 [  1941   7060   6220   6094   5588   3859   2029   1093    727     14]
 [  3713  17406  17714  19658  20332  14478   9539   4836   3506     14]
 [  1704   7479   8946  11319  12972  11298   7491   5073   5406      0]]
2022-05-08 20:15:53,264 - INFO - Epoch: 6 | Validation Loss: 89.1843
2022-05-08 20:30:30,014 - INFO - Custom bins confusion matrix:
2022-05-08 20:30:30,014 - INFO - [[1016201  844762  138429   51489   23443   11981    6483    3735    5705
      775]
 [ 361818  568141  157812   72184   34937   18308    9908    5709    8545
     1089]
 [ 146566  328840  129047   68746   36579   19959   11318    6814   10724
     1400]
 [  67020  195978   99322   59197   33340   19415   11714    6937   11555
     1552]
 [  34456  122450   74449   48990   29828   17911   10838    6819   11710
     1825]
 [  20203   79302   56032   40101   26054   16402   10074    6416   11627
     1844]
 [  12473   53284   43172   33431   22561   14525    9109    5895   11009
     1981]
 [   7725   35790   33524   27767   19418   12864    8506    5448   10669
     2030]
 [  18230   84985   97286   90779   68831   48216   33098   22794   47133
    10474]
 [   6600   33309   48843   54757   47713   36862   27177   20003   47425
    14139]]
2022-05-08 20:30:35,933 - INFO - Epoch: 7 | Train Loss: 91.9384
2022-05-08 20:33:12,746 - INFO - Custom bins confusion matrix:
2022-05-08 20:33:12,747 - INFO - [[225909 167533  32132  13518   6208   2743   1172    464    263      0]
 [ 78569 114399  34814  19150   9623   3998   1790    530    369      0]
 [ 30971  66343  27295  18010  10227   5047   2173    838    484      4]
 [ 14632  38528  19760  14832   9921   5148   2269   1027    660     10]
 [  7505  23019  14623  12436   8633   5059   2528   1066    661     12]
 [  4343  15820  10426   9833   7648   4563   2495   1205    808      7]
 [  2707  10075   7880   7637   6685   4116   2296   1121    935      8]
 [  1812   6955   6191   6034   5654   3894   2097   1108    858     22]
 [  3461  16708  17616  19594  20221  14801   9648   5101   4008     38]
 [  1619   7125   8863  11225  12671  11152   7832   4980   6219      2]]
2022-05-08 20:33:14,321 - INFO - Epoch: 7 | Validation Loss: 88.8325
2022-05-08 20:48:40,044 - INFO - Custom bins confusion matrix:
2022-05-08 20:48:40,045 - INFO - [[1016886  846612  136926   50833   23361   11753    6375    3745    5725
      787]
 [ 362212  569542  156809   71325   34558   18274   10172    5906    8562
     1091]
 [ 145600  330355  128626   68832   36348   20037   11367    6803   10652
     1373]
 [  65969  196637   99518   58970   33857   19263   11618    6923   11601
     1674]
 [  34074  122260   74730   48941   29782   18139   10929    6640   11861
     1920]
 [  20110   78603   56110   40790   25961   16030   10205    6564   11689
     1993]
 [  12383   52626   43003   33648   22964   14354    9244    6063   11078
     2077]
 [   7591   35468   33386   28044   19677   12709    8348    5499   10893
     2126]
 [  17508   84114   96089   90156   69004   48919   33581   22824   48612
    11019]
 [   6566   31656   47820   54106   47619   36875   27657   20282   48915
    15332]]
2022-05-08 20:48:46,004 - INFO - Epoch: 8 | Train Loss: 91.3210
2022-05-08 20:51:24,405 - INFO - Custom bins confusion matrix:
2022-05-08 20:51:24,406 - INFO - [[228549 166191  31399  13090   6064   2682   1226    451    290      0]
 [ 79797 114578  34035  18602   9429   3971   1798    635    397      0]
 [ 31368  66769  26987  17536   9933   5188   2168    925    507     11]
 [ 14917  38840  19463  14586   9677   5145   2372   1048    717     22]
 [  7635  23441  14431  12127   8409   4979   2656   1113    729     22]
 [  4402  16042  10322   9728   7350   4574   2551   1260    900     19]
 [  2761  10181   7839   7593   6443   4063   2396   1124   1052      8]
 [  1856   7061   6255   5877   5351   3814   2216   1206    968     21]
 [  3503  16839  18051  19070  19180  14780   9588   5574   4569     42]
 [  1652   7116   8866  11280  12256  10951   7734   5194   6619     20]]
2022-05-08 20:51:25,971 - INFO - Epoch: 8 | Validation Loss: 88.4512
2022-05-08 21:06:57,220 - INFO - Custom bins confusion matrix:
2022-05-08 21:06:57,221 - INFO - [[1017832  847577  134785   51016   23470   11773    6463    3647    5645
      795]
 [ 361348  571133  156011   71296   35044   18388    9767    5749    8642
     1073]
 [ 144330  332068  128639   68291   36466   20105   11387    6578   10639
     1490]
 [  65339  197168   99572   58783   33725   19389   11549    6922   11783
     1800]
 [  33559  122837   74339   49324   29879   17930   10994    6664   11805
     1945]
 [  19617   79085   56075   40339   25955   16287   10133    6750   11786
     2028]
 [  11920   52883   43114   33466   22936   14547    9212    5923   11263
     2176]
 [   7392   35517   33195   28135   19638   12932    8477    5523   10748
     2184]
 [  17151   82849   94904   90136   69713   49557   33548   23264   49247
    11457]
 [   6527   30591   46271   53858   47509   37252   27878   20383   50130
    16429]]
2022-05-08 21:07:03,051 - INFO - Epoch: 9 | Train Loss: 90.8233
2022-05-08 21:09:41,382 - INFO - Custom bins confusion matrix:
2022-05-08 21:09:41,383 - INFO - [[229604 165178  30865  13329   6178   2728   1281    496    283      0]
 [ 80040 114293  33366  18867   9517   4131   1883    720    425      0]
 [ 31323  66696  26527  17641  10001   5310   2277   1007    597     13]
 [ 14816  38781  19060  14745   9720   5137   2593   1100    809     26]
 [  7551  23375  14198  12104   8534   4889   2795   1261    809     26]
 [  4368  15996  10193   9576   7458   4585   2619   1310   1021     22]
 [  2735  10137   7713   7552   6534   3869   2590   1165   1156      9]
 [  1859   6989   6144   5815   5463   3709   2289   1194   1139     24]
 [  3518  16507  17607  19198  19021  14619   9587   5946   5125     68]
 [  1653   7080   8875  10874  12487  10621   7811   5237   6972     78]]
2022-05-08 21:09:43,005 - INFO - Epoch: 9 | Validation Loss: 88.2481
2022-05-08 21:09:43,017 - INFO - Experiment ended. Checkpoints stored =)
